{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import ensemble\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "import pandas, xgboost\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stopWord = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "######################################################## 1. Dataset preparation\n",
    "# load the dataset\n",
    "# train Data\n",
    "df_trainData = pd.read_csv(\"D:\\\\NTU MSIS\\\\H6751-TEXT & WEB MINING\\\\Kaggle Competition\\\\Data\\\\train.csv\")\n",
    "# test Data\n",
    "df_testData = pd.read_csv(\"D:\\\\NTU MSIS\\\\H6751-TEXT & WEB MINING\\\\Kaggle Competition\\\\Data\\\\test.csv\")\n",
    "\n",
    "# Remove HTML tag\n",
    "df_trainData['Comment'] = df_trainData['Comment'].str.replace('<.*?>', '', case=False)\n",
    "# Remove stop word\n",
    "df_trainData['Comment'] = df_trainData['Comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopWord)]))\n",
    "# Remove hyper link\n",
    "df_trainData['Comment'] = df_trainData['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "# Remove sepecial characer\n",
    "df_trainData['Comment'] = df_trainData['Comment'].replace({'[^A-Za-z]+':' '}, regex=True)\n",
    "# Remove Single character\n",
    "df_trainData['Comment'] = df_trainData['Comment'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 1 ]))\n",
    "# Lemmatization  \n",
    "df_trainData['Comment'] = df_trainData['Comment'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))  \n",
    "\n",
    "# Remove HTML tag\n",
    "df_testData['Comment'] = df_testData['Comment'].str.replace('<.*?>', '', case=False)\n",
    "# Remove stop word\n",
    "df_testData['Comment'] = df_testData['Comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopWord)]))\n",
    "# Remove hyper link\n",
    "df_testData['Comment'] = df_testData['Comment'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "# Remove sepecial characer\n",
    "df_testData['Comment'] = df_testData['Comment'].replace({'[^A-Za-z]+':' '}, regex=True)\n",
    "# Remove Single character\n",
    "df_testData['Comment'] = df_testData['Comment'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 1 ]))\n",
    "# Lemmatization  \n",
    "df_testData['Comment'] = df_testData['Comment'].apply(lambda x: ' '.join([lemmatizer.lemmatize(word) for word in x.split()]))  \n",
    "\n",
    "# prepare cross validation\n",
    "X = df_trainData['Comment']\n",
    "y = df_trainData['Outcome']\n",
    "\n",
    "kf = KFold(n_splits=2)\n",
    "kf.get_n_splits(df_trainData['Comment'])\n",
    "\n",
    "def train_model(classifier, feature_vector_train, label, feature_vector_valid):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    return metrics.accuracy_score(predictions, valid_y)\n",
    "\n",
    "x = 0\n",
    "for train_index, test_index in kf.split(X):\n",
    "    x += 1\n",
    "    print(\"************************************Kfold: \" + str(x))\n",
    "    train_x, valid_x = X[train_index], X[test_index]\n",
    "    train_y, valid_y = y[train_index], y[test_index]\n",
    "    \n",
    "    # create a dataframe using texts and lables\n",
    "    trainDF = pandas.DataFrame()\n",
    "    trainDF['text'] = df_trainData['Comment']\n",
    "\n",
    "    ######################################################## 2. Feature Engineering\n",
    "    ############ 2.1 Count Vectors as features\n",
    "    # create a count vectorizer object \n",
    "    count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
    "    count_vect.fit(trainDF['text'])\n",
    "    # transform the training and validation data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(train_x)\n",
    "    xvalid_count =  count_vect.transform(valid_x)\n",
    "    \n",
    "    ############ 2.2 TF-IDF Vectors as features\n",
    "    # word level tf-idf\n",
    "    tfidf_vect = TfidfVectorizer(min_df = 5,  max_df = 0.8, sublinear_tf = True, use_idf = True, analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "    tfidf_vect.fit(trainDF['text'])\n",
    "    xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "    xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "    # ngram level tf-idf \n",
    "    tfidf_vect_ngram = TfidfVectorizer(min_df = 5,  max_df = 0.8, sublinear_tf = True, use_idf = True, analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "    tfidf_vect_ngram.fit(trainDF['text'])\n",
    "    xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "    xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "    \n",
    "    ######################################################## 3. Model Building\n",
    "    ############3.1 Naive Bayes\n",
    "    # Naive Bayes on Count Vectors\n",
    "    accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
    "    print(\"........................................Naive Bayes, Count Vectors: \", accuracy)\n",
    "    # Naive Bayes on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print(\"........................................Naive Bayes, WordLevel TF-IDF: \", accuracy)\n",
    "    # Naive Bayes on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print(\"........................................Naive Bayes, N-Gram Vectors: \", accuracy)\n",
    "    print(\"\")\n",
    "    \n",
    "    ############3.2 Linear Classifier\n",
    "    # Linear Classifier on Count Vectors\n",
    "    accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
    "    print(\"........................................Logistic Regression, Count Vectors: \", accuracy)\n",
    "    # Linear Classifier on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print(\"........................................Logistic Regression, WordLevel TF-IDF: \", accuracy)\n",
    "    # Linear Classifier on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print(\"........................................Logistic Regression, N-Gram Vectors: \", accuracy)\n",
    "    print(\"\")\n",
    "    \n",
    "    ############3.3 SVM Model\n",
    "    # SVM on Count Vectors\n",
    "    accuracy = train_model(svm.SVC(), xtrain_count, train_y, xvalid_count)\n",
    "    print(\"........................................SVM, Count Vectors: \", accuracy)\n",
    "    # SVM on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(svm.SVC(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print(\"........................................SVM, WordLevel TF-IDF: \", accuracy)\n",
    "    # SVM on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print(\"........................................SVM, N-Gram Vectors: \", accuracy)\n",
    "    print(\"\")\n",
    "    \n",
    "    ############3.4 Bagging Model\n",
    "    # RandomForest on Count Vectors\n",
    "    accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
    "    print(\"........................................RandomForest, Count Vectors: \", accuracy)\n",
    "    # RandomForest on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
    "    print(\"........................................RandomForest, WordLevel TF-IDF: \", accuracy)\n",
    "    # RandomForest on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
    "    print(\"........................................RandomForest, N-Gram Vectors: \", accuracy)\n",
    "    print(\"\")\n",
    "    \n",
    "    ############3.5 Boosting Model\n",
    "    # Gradient Boosting on Count Vectors\n",
    "    accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
    "    print(\"........................................Xgb, Count Vectors: \", accuracy)\n",
    "    # Gradient Boosting on Word Level TF IDF Vectors\n",
    "    accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
    "    print(\"........................................Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "    # Gradient Boosting on Ngram Level TF IDF Vectors\n",
    "    accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram.tocsc())\n",
    "    print(\"........................................Xgb, N-Gram Vectors: \", accuracy)\n",
    "    print(\"\")\n",
    "\n",
    "print('------ DONE ------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
